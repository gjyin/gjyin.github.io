
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="./cagnet_files/jemdoc.css" type="text/css">
<title>Project Page of Context and Attribute Grounded Dense Captioning</title>

<style type="text/css">
BODY {
	TEXT-ALIGN: center; PADDING-BOTTOM: 0px; PADDING-LEFT: 0px; PADDING-RIGHT: 0px; FONT: 100% "Times New Roman", Times, serif; BACKGROUND: #ffffff; COLOR: #000; PADDING-TOP: 0px
}
.oneColFixCtr #container {
	BORDER-BOTTOM: #000000 1px ;
	TEXT-ALIGN: left;
	BORDER-LEFT: #000000 1px ;
	MARGIN: 0px auto;
	WIDTH: 1000px;
	BACKGROUND: #ffffff;
	BORDER-TOP: #000000 1px ;
	BORDER-RIGHT: #000000 1px 
}
.oneColFixCtr #mainContent {
	PADDING-BOTTOM: 0px; PADDING-LEFT: 20px; PADDING-RIGHT: 20px; PADDING-TOP: 0px
}
.style3 {
	FONT-SIZE: small
}
.style5 {
	FONT-SIZE: large; FONT-WEIGHT: bold
}
.style6 {
	FONT-SIZE: large
}
.style7 {
	TEXT-DECORATION: none
}
.style8 {
	COLOR: #000000
}
.style9 {
	COLOR: #000080
}
.style10 {
	MARGIN-TOP: 5pt; MARGIN-BOTTOM: 5pt; FONT-SIZE: medium
}
.style11 {
	MARGIN-TOP: 5pt; TEXT-INDENT: 15px; MARGIN-BOTTOM: 5pt; FONT-SIZE: medium
}
.style12 {
	MARGIN-LEFT: 12pt; FONT-SIZE: medium; MARGIN-RIGHT: 12pt}
.code {
	FONT-FAMILY: "Courier New", Courier, monospace; FONT-SIZE: 15px
}
.codeline {
	MARGIN-TOP: 5pt; TEXT-INDENT: 15px; FONT-FAMILY: "Courier New", Courier, monospace; MARGIN-BOTTOM: 5pt; FONT-SIZE: 15px
}
.DivCode {
	BORDER-BOTTOM: #333 1px dashed; BORDER-LEFT: #333 1px dashed; WIDTH: 800px; BACKGROUND: #ffd; MARGIN-LEFT: 10pt; FONT-SIZE: medium; BORDER-TOP: #333 1px dashed; BORDER-RIGHT: #333 1px dashed
}
.auto-style5 {
	MARGIN-TOP: 3pt; MARGIN-BOTTOM: 3pt; FONT-SIZE: 100%
}
.STYLE15 {FONT-SIZE: large; FONT-WEIGHT: bold; color: #FF0000; }
#motion_channel {
  padding-right: 0px;
  padding-left: 20px;
  float: right;
  padding-bottom: 20px;
  padding-top: 0px;
}
</style>

<meta name="GENERATOR" content="MSHTML 8.00.7601.17744"></head>
<body class="oneColFixCtr">
<div id="container">
<div style="MARGIN-BOTTOM: 0pt" id="mainContent">
  <h1 style="MARGIN-TOP: 20pt" align="center"><a style="COLOR: #000; TEXT-DECORATION: none">Context and Attribute Grounded Dense Captioning</a></h1>
  <p style="MARGIN-TOP: 20pt" class="style6" align="center">
    <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://gjyin91.github.io/" target="_blank">Guojun Yin</a><sup>1,2</sup></span>,
    <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://lucassheng.github.io/" target="_blank">Lu Sheng</a><sup>2,4</sup></span>,
    <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://eeis.ustc.edu.cn/2016/0422/c2615a19449/page.htm" target="_blank">Bin Liu</a><sup>1</sup></span>,
    <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://eeis.ustc.edu.cn/2010/0825/c2648a19486/page.htm" target="_blank">Nenghai Yu</a><sup>1</sup></span>,
    <span class="style6" style="MARGIN-TOP: 20pt"><a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a><sup>2</sup></span>,
  	and <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://amandajshao.github.io/" target="_blank">Jing Shao</a><sup>3</sup></span> </p>
  <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style6" align="center"><sup>1</sup>University of Science and Technology of China,  <sup>2</sup>The Chinese University of Hong Kong, <br><sup>3</sup>SenseTime Research, <sup>4</sup>Beihang University </p>

<p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style6" align="center"><span class="style8">
  [<a href="https://arxiv.org/pdf/1904.01410.pdf" target="_blank">PDF</a>]
  [<a href="https://github.com/gjyin91/CAG-Net" target="_blank">Code</a>]
  [<a href="https://gjyin91.github.io/" target="_blank">Homepage</a>]
</span>
</p>
<p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p>
</div>

<div class="style12">
<p align="center"><img src="cagnet_files/s1.jpg" width="450" height="350">   &nbsp;&nbsp;&nbsp;&nbsp;        <img src="cagnet_files/s2.jpg" width="450" height="350"></p>
</div>

<div class="style12">
  <h2>Introduction  </h2>
  <p align="left">Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to state-of-the-art methods.</p>

  <p align="center"><img src="cagnet_files/fig_1_v2.jpg" width="600" height="300"> </p>

  <p align="left">The contributions of this work: </p>
  <ul>
    <li> We design a context and attribute grounded dense captioning model that permits multi-scale (i.e., local, neighboring, global) contextual information sharing and message passing, in which the knowledge integration is built on a non-local similarity graph among instances in the input image. </li>
    <li> A coarse-to-fine linguistic attribute supervision is proposed to enhance the discriminativeness of the generated captions, in which the ground-truth hierarchical linguistic attributes are matched to the predicted keywords through a novel coarse-to-fine manner. </li>
    <li> Extensive experiments demonstrate the effectiveness of the proposed CAG-Net model on the challenging large-scale VG dataset.</li>
  </ul>
  <!-- <p align="center"><img src="cagnet_files/fig_all_01.jpg" width="480" height="180"></p> -->
<div align="center"></div>
<div></div>
</div>



<div class="style12">
<!-- <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p> -->
<h2>CAG-Net</h2>
<p>
  In this paper, we propose a novel end-to-end dense image captioning framework, named as Context and Attribute Grounded Dense Captioning (CAG-Net). We first learn visual features of the input image by a CNN model as the way adopted by Faster RCNN, and obtain the semantic features. Such semantic features are used to generate a set of candidate regions (ROIs) by a Region Proposal Network (RPN). Based on these ROI features, we introduce a Contextual Feature Extractor (CFE) which generates the global, neighboring and local (i.e., target itself) cues. The neighboring cues are collected by establishing a similarity graph between the target ROI and the neighboring ROIs. The multi-scale contextual cues, broadcast in parallel, are fused by an Attribute Grounded Caption Generator (AGCG) which employs multiple LSTM cells. To generate rich and fine-grained descriptions and reinforce the coarse-to-fine procedure of description generation, we adopt an auxiliary supervision, Linguistic Attributes, hierarchically upon the outputs of the sequential LSTM cells. The proposed model is trained to minimize both the sentence loss as well as the binary cross-entropy losses (attribute losses) for caption generation. 
<p align="center"><img src="cagnet_files/fig_pipeline_v3.jpg" width="800" height="380"></p>
</p>
</div>


<div class="style12">
<!-- <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p> -->
<h2>Contextual Feature Extractor</h2>
<p> 
We design a similarity graph based on region-level (i.e., ROI-level) for neighboring ROIs aggregation, inspired by pixel-level non-local operations. Non-local means
has been often used as a filter by computing a weighted mean of all pixels in an image, which allows pixels to contribute to the filtered response based on the patch appearance similarity. Similarly, neighboring ROIs with similar semantic appearance are supposed to contribute more on the feature extraction for the target local instance.
</p>
<p align="center"><img src="cagnet_files/fig_context_mining.jpg" width="500" height="240"></p>


</div>


<div class="style12">
  <h2>Contextual Cue Integrator</h2>
  <p>
    The contextual cue integrator adopts multiple LSTM cells to hierarchically integrate the multi-scale contextual features into the localized features. The local, neighboring and global features are spread through in the LSTM cells so as to generate contextaware descriptions for the target ROI. These descriptions are fused together for the final captioning of the target region at each time step of LSTM. The local branch can be regarded as the backbone for the target while the global and neighboring branches are grouped as multi-scale contextual cues to provide complementary guidances. Therefore, the contextual cues are adaptively combined at first, and they are then added to the local branch via a second adaptive fusion.

  </p>
<p align="center"><img src="cagnet_files/fig_unrolled_rnn_v2.jpg" width="500" height="600"></p>


</div>


<div class="style12">
<!-- <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p> -->
<h2>Attribute Grounded Coarse-to-Fine Generator</h2>
<p> 
It is challenging in generating rich and accurate descriptions just by the sequential LSTMs. To this end, we increase its representative power by introducing a coarse-to-fine caption generation procedure with sequential LSTM cells, i.e., coarse stage and refined stage supervised with the auxiliary hierarchical linguistic attribute losses.
</p>

<p>
The subsequent LSTM layer (refined stage) is supposed to serve as the fine-grained decoders for the coarse regional descriptions generated by the preceding one
(coarse stage). The hidden vectors of LSTM cells produced by the coarse stage are taken as the disambiguating cues to the refined stage. The outputs of global and neighboring branches at the coarse stage are used as the inputs of the respective branches directly at the refined stage. The adaptive fusion of these three branches at the coarse stage is fed as the input at the refined stage. Meanwhile, these vectors are used for coarse-level attribute prediction. The final outputs of the word decoder at the refined stage are the generated descriptions for the target region. Meanwhile, these outputs are used for the fined-level attribute prediction as well.


</p>

<p> 
These linguistic attributes are predicted from the outputs of the LSTMs during the training procedure and the unsolved problem here is how to get the ground-truth linguistic attributes. In our work, the hierarchical linguistic attributes are obtained by itemizing the sentences in the training split with natural language processing toolkit (NLTK).
</p>
<p align="center"><img src="cagnet_files/fig_sentence_loss_v3.jpg" width="700" height="300"></p>
</div>


<div class="style12">
<h2>Code</h2>
<p>Please refer to the <a href="https://github.com/gjyin91/CAG-Net">GitHub repository</a> for more details.</p>
</div>

<div class="style12">
<h2>Reference</h2>
<!-- <blockquote> -->
  <p align="left">If you use our code or model, please cite our papers.</p>
  <p align="left" class="DivCode">Yin, Guojun, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, and Jing Shao. "Context and Attribute Grounded Dense Captioning", in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2019. &nbsp; </p>
  <!-- </blockquote> -->
  
</div>


<div class="style12">
  <h2>Contact Me </h2>
  If you have any questions, please feel free to contact me (gjyin91@gmail.com or gjyin@mail.ustc.edu.cn).
</div>


<div class="style12">
  <p class="pull-right">
    <a href="cagnet.html">Back to top</a>
  </p>
</div>
<div>
<p style="MARGIN-TOP: 30px; MARGIN-BOTTOM: 30px" class="style11" align="left">Last 
update: June 11, 2019 </p>
</div></div>
<div><object id="ClCache" click="sendMsg" host="" width="0" height="0"></object></div></body></html>