
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="./sdgan_files/jemdoc.css" type="text/css">
<title>Project Page of Semantics Disentangling for Text-to-Image Generation</title>

<style type="text/css">
BODY {
	TEXT-ALIGN: center; PADDING-BOTTOM: 0px; PADDING-LEFT: 0px; PADDING-RIGHT: 0px; FONT: 100% "Times New Roman", Times, serif; BACKGROUND: #ffffff; COLOR: #000; PADDING-TOP: 0px
}
.oneColFixCtr #container {
	BORDER-BOTTOM: #000000 1px ;
	TEXT-ALIGN: left;
	BORDER-LEFT: #000000 1px ;
	MARGIN: 0px auto;
	WIDTH: 1000px;
	BACKGROUND: #ffffff;
	BORDER-TOP: #000000 1px ;
	BORDER-RIGHT: #000000 1px 
}
.oneColFixCtr #mainContent {
	PADDING-BOTTOM: 0px; PADDING-LEFT: 20px; PADDING-RIGHT: 20px; PADDING-TOP: 0px
}
.style3 {
	FONT-SIZE: small
}
.style5 {
	FONT-SIZE: large; FONT-WEIGHT: bold
}
.style6 {
	FONT-SIZE: large
}
.style7 {
	TEXT-DECORATION: none
}
.style8 {
	COLOR: #000000
}
.style9 {
	COLOR: #000080
}
.style10 {
	MARGIN-TOP: 5pt; MARGIN-BOTTOM: 5pt; FONT-SIZE: medium
}
.style11 {
	MARGIN-TOP: 5pt; TEXT-INDENT: 15px; MARGIN-BOTTOM: 5pt; FONT-SIZE: medium
}
.style12 {
	MARGIN-LEFT: 12pt; FONT-SIZE: medium; MARGIN-RIGHT: 12pt}
.code {
	FONT-FAMILY: "Courier New", Courier, monospace; FONT-SIZE: 15px
}
.codeline {
	MARGIN-TOP: 5pt; TEXT-INDENT: 15px; FONT-FAMILY: "Courier New", Courier, monospace; MARGIN-BOTTOM: 5pt; FONT-SIZE: 15px
}
.DivCode {
	BORDER-BOTTOM: #333 1px dashed; BORDER-LEFT: #333 1px dashed; WIDTH: 800px; BACKGROUND: #ffd; MARGIN-LEFT: 10pt; FONT-SIZE: medium; BORDER-TOP: #333 1px dashed; BORDER-RIGHT: #333 1px dashed
}
.auto-style5 {
	MARGIN-TOP: 3pt; MARGIN-BOTTOM: 3pt; FONT-SIZE: 100%
}
.STYLE15 {FONT-SIZE: large; FONT-WEIGHT: bold; color: #FF0000; }
#motion_channel {
  padding-right: 0px;
  padding-left: 20px;
  float: right;
  padding-bottom: 20px;
  padding-top: 0px;
}
</style>

<meta name="GENERATOR" content="MSHTML 8.00.7601.17744"></head>
<body class="oneColFixCtr">
<div id="container">
<div style="MARGIN-BOTTOM: 0pt" id="mainContent">
  <h1 style="MARGIN-TOP: 20pt" align="center"><a style="COLOR: #000; TEXT-DECORATION: none">Semantics Disentangling for Text-to-Image Generation</a></h1>
  <p style="MARGIN-TOP: 20pt" class="style6" align="center">
  	<span class="style6" style="MARGIN-TOP: 20pt"><a href="https://gjyin91.github.io/" target="_blank">Guojun Yin</a><sup>1,2</sup></span>,
  	<span class="style6" style="MARGIN-TOP: 20pt"><a href="https://eeis.ustc.edu.cn/2016/0422/c2615a19449/page.htm" target="_blank">Bin Liu</a><sup>1</sup></span>,
  	<span class="style6" style="MARGIN-TOP: 20pt"><a href="https://lucassheng.github.io/" target="_blank">Lu Sheng</a><sup>2,4</sup></span>,
  	<span class="style6" style="MARGIN-TOP: 20pt"><a href="https://eeis.ustc.edu.cn/2010/0825/c2648a19486/page.htm" target="_blank">Nenghai Yu</a><sup>1</sup></span>,
  	<span class="style6" style="MARGIN-TOP: 20pt"><a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank">Xiaogang Wang</a><sup>2</sup></span>,
  	and <span class="style6" style="MARGIN-TOP: 20pt"><a href="https://amandajshao.github.io/" target="_blank">Jing Shao</a><sup>3</sup></span> </p>
  <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style6" align="center"><sup>1</sup>University of Science and Technology of China,  <sup>2</sup>The Chinese University of Hong Kong, <br><sup>3</sup>SenseTime Research, <sup>4</sup>Beihang University </p>

<p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style6" align="center"><span class="style8">
  [<a href="https://arxiv.org/pdf/1904.01480.pdf" target="_blank">PDF</a>]
  [<a href="https://github.com/gjyin91/SDGAN" target="_blank">Code</a>]
  [<a href="https://gjyin91.github.io/" target="_blank">Homepage</a>]
  [<a href="#oralvideo">Video</a>]
</span>
</p>
<p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p>
</div>

<div class="style12">
<p align="center"><img src="sdgan_files/fig_result_component.jpg" width="900" height="300"> 
</div>

<div class="style12">
  <h2>Introduction  </h2>
  <p align="left">Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.</p>

  <p align="center"><img src="sdgan_files/fig_1.jpg" width="500" height="400"> </p>

  <p align="left">The contributions of this work: </p>
  <ul>
    <li> <b> Distill Semantic Commons from Text ---</b>The proposed SDGAN distills semantic commons from the linguistic descriptions, based on which the generated images can keep generation consistency under expression variants. To our best knowledge, it is the first time to introduce the Siamese mechanism into the cross-modality generation. </li>
    <li> <b> Retain Semantic Diversities & Details from Text ---</b> To complement the Siamese mechanism that may lose unique semantic diversities, we design an enhanced visual-semantic embedding method by reformulating the batch normalization layer with the instance linguistic cues. The linguistic embedding can further guide the visual pattern synthesis for fine-grained image generation. </li>
    <li> The proposed SD-GAN achieves the state-of-the-art performance on the CUB-200 bird dataset and MS-COCO dataset for text-to-image generation.</li>
  </ul>
  <!-- <p align="center"><img src="sdgan_files/fig_all_01.jpg" width="480" height="180"></p> -->
<div align="center"></div>
<div></div>
</div>



<div class="style12">
<!-- <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p> -->
<h2>Semantics Disentangling Generative Adversarial Network (SD-GAN)</h2>
<p>
  In this paper, we propose a new cross-modal generation network named as <b>Semantics Disentangling Generative Adversarial Network (SD-GAN)</b> for text-to-image generation. It aims at distilling the semantic commons from texts for image generation consistency and meanwhile retaining the semantic diversities & details for fine-grained image generation: (1) Taking the advantages of Siamese structure, the generated images are not only based on the input description at the current branch, but also influenced by the description at the other branch. In other words, the Siamese structure distills the common semantics from texts to handle the generation deviation under the expression variants. (2) To generate fine-grained visual patterns, the model also needs to retain the detailed and diverse semantics of the input texts. We modulate neural activations with linguistic cues by the proposed Semantic-Conditioned Batch Normalization (SCBN). 
<p align="center"><img src="sdgan_files/fig_pipeline.jpg" width="900" height="380"></p>
</p>
</div>


<div class="style12">
<!-- <p style="MARGIN-TOP: 5pt; margin-bottom: 5pt;" class="style3" align="center">&nbsp;</p> -->
<h2>Siamese structure with Contrastive Loss </h2>
<p> 
Although existing methods achieved excellent performances on high-resolution image generation, the generation deviations from language expression variants still pose great challenges for the text-semantic image generation. To address the issues, the proposed SD-GAN adopts a Siamese structure for distilling textual semantic information for the cross-domain generation. The contrastive loss is adopted for minimizing the distance of the fake images generated from two descriptions of the same groundtruth image while maximizing those of different groundtruth images. During the training stage, the generated image is influenced
by the texts from both two branches.
</p>
<p align="center"><img src="sdgan_files/fig_discriminator_v2.jpg" width="500" height="500"></p>

<p> 
We adopt hierarchical stages from lowresolution to high-resolution for the photo-realistic image generation. Given the sentence feature from the text encoder
E and a noise vector z sampled from a standard normal distribution, the low resolution image is generated at the initial stage. The following stage uses the output of the former stage as well as the sentence feature to generate the image with higherresolution. </p>
<p align="center"><img src="sdgan_files/fig_generator.jpg" width="450" height="250"></p>
</div>


<div class="style12">
  <h2>Semantic-Conditioned Batch Normalization (SCBN)</h2>
  <p>
    In this work, we consider the linguistic concepts as the kernels of visual representations for cross-domain generation from linguistic to vision. Inspired by the instance normalization in the existing works, we modulate the conditional batch normalization with the linguistic cues from the natural language descriptions, defined as  <b>Semantic-Conditioned Batch Normalization (SCBN)</b>. The purpose of SCBN is to reinforce the visual-semantic embedding in the feature maps of the generative networks. It enables the linguistic embedding to manipulate the visual feature maps by scaling them up or down, negating them, or shutting them off, etc. It complements to the Siamese structure which only focuses on distilling semantic commons but ignore the unique semantic diversities in the text.

  </p>
<p align="center"><img src="sdgan_files/fig_cbn.jpg" width="600" height="300"></p>
</div>


<div class="style12">
<h2><a id="oralvideo"> Oral </a> </h2>
&nbsp;
<p align="center"><iframe width="960" height="540" src="sdgan_files/sdgan.mp4" frameborder="0" allowfullscreen>
&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;
&amp;amp;amp;amp;lt;p&amp;amp;amp;amp;gt;&amp;amp;amp;amp;amp;nbsp;&amp;amp;amp;amp;lt;/p&amp;amp;amp;amp;gt;
</iframe></p> 

<p align="center">&nbsp;</p>
<div align="center"></div>
<div></div>
</div>

<div class="style12">
<h2>Code</h2>
<p>Please refer to the <a href="https://github.com/gjyin91/SDGAN">GitHub repository</a> for more details.</p>
</div>

<div class="style12">
<h2>Reference</h2>
<!-- <blockquote> -->
  <p align="left">If you use our code or model, please cite our papers.</p>
  <p align="left" class="DivCode">Yin, Guojun, Bin Liu, Lu Sheng, Nenghai Yu, Xiaogang Wang, and Jing Shao. "Semantics Disentangling for Text-to-Image Generation", in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019). &nbsp; </p>
  <!-- </blockquote> -->
  
</div>


<div class="style12">
  <h2>Contact Me </h2>
  If you have any questions, please feel free to contact me (gjyin91@gmail.com or gjyin@mail.ustc.edu.cn).
</div>


<div class="style12">
  <p class="pull-right">
    <a href="sdgan.html">Back to top</a>
  </p>
</div>
<div>
<p style="MARGIN-TOP: 30px; MARGIN-BOTTOM: 30px" class="style11" align="left">Last 
update: June 11, 2019 </p>
</div></div>
<div><object id="ClCache" click="sendMsg" host="" width="0" height="0"></object></div></body></html>